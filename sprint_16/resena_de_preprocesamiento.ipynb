{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEb2BfcuFggz"
   },
   "source": [
    "# Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones\n",
    "\n",
    "Adapta el código de preprocesamiento de las lecciones a la tarea de clasificación de reseñas:\n",
    "\n",
    "Descarga el archivo con las reseñas /datasets/imdb_reviews_small.tsv.\n",
    "Tokeniza cada reseña y considera que en el siguiente paso puedes tener hasta 512 tokens.\n",
    "Encuentra la longitud mínima y máxima de los vectores después de la tokenización.\n",
    "Aplica la técnica de relleno a los vectores y crea máscaras de atención para distinguir los tokens de los valores de relleno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rs6HRbhWFgg0",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WZW-pm6Fgg0"
   },
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS0vdd5mFgg0"
   },
   "source": [
    "Carga los datos de texto del archivo 'imdb_reviews_small.tsv' file.\n",
    "\n",
    "Es un archivo de valores separados por tabuladores (TSV), lo cual significa que cada uno de los campos están separados por tabuladores (en lugar de comas, como has visto en otros ejercicios de TripleTen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nimxZzEFgg0"
   },
   "outputs": [],
   "source": [
    "data = # <tu código aquí>\n",
    "#/datasets/imdb_reviews_small.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0emU20IFgg0"
   },
   "source": [
    "# Tokenizador BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_i0JMPAFgg1"
   },
   "source": [
    "Crea el tokenizador BERT a partir de un modelo previamente entrenado que se llama 'bert-base-uncased' en transformadores. Puedes consultar rápidamente una descripción general [aquí](https://huggingface.co/transformers/pretrained_models.html), Y para obtener más detalles, puedes leer [aquí](https://huggingface.co/bert-base-uncased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObHWqouUFgg1",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEoVM_tbFgg1"
   },
   "source": [
    "Hay un ejemplo de cómo obtener tokens para un solo texto dado.\n",
    "\n",
    "Puedes usarlo para procesar todos los datos que cargaste anteriormente. Como ya hay muchos textos, y es probable que los proceses en un bucle, las longitudes mínimas/máximas de los vectores se pueden calcular de dos formas: dentro de un bucle o después de un bucle.\n",
    "\n",
    "En el último caso, los vectores de identificadores numéricos de tokens (`ids`) y máscaras de atención (`attention_mask`) se deben almacenar en dos listas separadas. Se pueden llamar `ids_list` y `atencion_mask_list`, respectivamente. El primer caso te permite evitar la creación de esas listas, a menos que desees utilizarlas con otra finalidad, por ejemplo, para propagarlas en un modelo BERT. No se requiere en esta tarea, pero se requerirá en el proyecto.\n",
    "\n",
    "Teniendo en cuenta lo anterior, es posible que desees combinar ambas formas para calcular las longitudes mínimas/máximas de los vectores para tokens y máscaras de atención, y conservar el resultado del tokenizador para su posterior procesamiento. Solo considera que no tiene mucho sentido mantener vectores de más de 512 elementos, ya que esta es la longitud máxima de vectores que BERT puede aceptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULoht5CBFgg1",
    "tags": [
     "90174557-ccbc-4d19-b780-5988d67a3706"
    ]
   },
   "outputs": [],
   "source": [
    "# textos a tokens\n",
    "text = 'Es muy práctico utilizar transformadores'\n",
    "\n",
    "# añadiendo este truco para suprimir avisos de salidas demasiado largas\n",
    "# normalmente no necesitamos eso, pero en este caso queremos explorar\n",
    "# cuál es la longitud máxima de ID para nuestro conjunto de reseñas\n",
    "# por eso no truncamos la salida (ids) de max_length\n",
    "# con los parámetros max_length=max_length y truncation=True\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "ids = tokenizer.encode(text.lower(), add_special_tokens=True)\n",
    "\n",
    "# padding (agregar ceros al vector para hacer que su longitud sea igual a n)\n",
    "n = 512\n",
    "padded = np.array(ids[:n] + [0]*(n - len(ids)))\n",
    "\n",
    "# creación de la máscara de atención para distinguir los tokens que nos interesan\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC_yLLkgFgg1",
    "outputId": "bf51c606-70fe-4d80-8db3-6858932abf0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2009, 2003, 2200, 18801, 2000, 2224, 19081, 102]\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWexyCiAFgg1",
    "outputId": "dd0ce914-79b9-4660-a27a-1f526d88a5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  2009  2003  2200 18801  2000  2224 19081   102     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksB1bKGLFgg1",
    "outputId": "8a15b602-03b3-4592-d0d3-514e2494cb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD4l29zOFgg2"
   },
   "source": [
    "Escribe tu código para tokenizar los datos de texto cargados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNS4vN2FFgg2"
   },
   "outputs": [],
   "source": [
    "def tokenize_with_bert(texts):\n",
    "\n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    # texto para las ID de tokens completadas con sus máscaras de atención\n",
    "    min_tokenized_text_length = 1e7  # Comience con un valor grande\n",
    "    max_tokenized_text_length = 0    # Empezar con 0\n",
    "\n",
    "    # Iterar a través de cada texto en la lista de entrada\n",
    "    # <tu código aquí>\n",
    "\n",
    "      # Suprimir las advertencias sobre salidas prolongadas del tokenizador. esto es util\n",
    "      # para explorar la distribución de longitudes de texto *sin* truncamiento, pero\n",
    "      # debe eliminarse o comentarse para un uso normal cuando se desea truncarlo.\n",
    "      logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "      # Tokenice el texto de entrada utilizando el tokenizador BERT.  Convertir a minúsculas\n",
    "      # y agregue tokens especiales ([CLS] y [SEP]).\n",
    "      ids = tokenizer.encode(# <tu código aquí>)\n",
    "\n",
    "      # Calcule y actualice las longitudes mínimas y máximas de texto tokenizado.\n",
    "\n",
    "      if ids_len\n",
    "        # <tu código aquí>\n",
    "      elif ids_len\n",
    "\n",
    "      # Truncar los ID de los tokens si superan max_length.  Los modelos BERT tienen un límite\n",
    "      # en la longitud de la secuencia de entrada.\n",
    "      # <tu código aquí>\n",
    "\n",
    "      # Rellene los ID de los tokens hasta max_length con 0.\n",
    "      # <tu código aquí>\n",
    "\n",
    "      # Crea una máscara de atención. 1 para tokens reales, 0 para tokens de relleno.\n",
    "\n",
    "      # Agregue las identificaciones acolchadas y la máscara de atención a sus respectivas listas.\n",
    "      ids_list.append(padded)\n",
    "      attention_mask_list.append(attention_mask)\n",
    "\n",
    "    print(f'La longitud mínima de los vectores: {min_tokenized_text_length}')\n",
    "    print(f'La longitud máxima de los vectores: {max_tokenized_text_length}')\n",
    "\n",
    "    return ids_list, attention_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP1pTBfCFgg2"
   },
   "source": [
    "Ejecuta el tokenizador para todos los datos. Puede tomar un poco de tiempo como"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv0oGP0lFgg2"
   },
   "outputs": [],
   "source": [
    "ids_list, attention_mask_list = tokenize_with_bert(texts=data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmwbyHksJ-en"
   },
   "source": [
    "Veamos algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb3xSjLyFgg2"
   },
   "outputs": [],
   "source": [
    "# cuenta elementos distintos de cero\n",
    "ids_array = np.count_nonzero(np.array(ids_list), axis=1)\n",
    "# almacenar el índice de la revisión que tiene la menor cantidad de tokens después de la tokenización\n",
    "short_review_idx = np.argmin(np.count_nonzero(np.array(ids_list), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvcfQJ5GLC3K"
   },
   "outputs": [],
   "source": [
    "# la reseña más corta\n",
    "data['review'].iloc[short_review_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGARhpgKOTSW"
   },
   "outputs": [],
   "source": [
    "# incorporación de la reseña más corta\n",
    "ids_list[short_review_idx][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eZacqBIOT2w"
   },
   "outputs": [],
   "source": [
    "# Revisión más breve integrada basada en BERT\n",
    "tokenizer.decode(ids_list[short_review_idx][:50], clean_up_tokenization_spaces=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
